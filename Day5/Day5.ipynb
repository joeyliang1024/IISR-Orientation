{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bmFmGd3GLeET"
   },
   "source": [
    "# Fine-tuning a LLM(Large Language Model) with LoRA\n",
    "\n",
    "<!-- 在本次練習，我們要微調一個大型語言模型，具體來說，我們會將 Meta 所預訓練的語言模型 LLaMA 微調在一份對話資料集 alpaca，讓模型變得像是聊天機器人\n",
    "\n",
    "而本次將使用的 LLaMA 模型為 7B 的版本，其擁有 70 億個模型參數，是一個相當大的模型，為了能夠順利訓練，我們還會使用 [LoRA](https://arxiv.org/abs/2106.09685) 以及 [INT8](https://arxiv.org/abs/2208.07339) 來進行訓練\n",
    "\n",
    "另外，因為訓練需要花上很長的時間，也沒有特別對超參數進行調整，如果訓練結果不好的話不需太在意，本次練習旨在 PyTorch Lightning 及 LLM 訓練的實作 -->\n",
    "\n",
    "In this exercise, we will fine-tune a large-scale language model. Specifically, we will fine-tune Meta's pre-trained language model LLaMA on a dialogue dataset called alpaca, aiming to make the model behave like a chatbot.\n",
    "\n",
    "The LLaMA model used for this exercise is the 7B version, which contains 7 billion model parameters, making it a considerably large model. To ensure a smooth training process, we will also utilize [LoRA](https://arxiv.org/abs/2106.09685) and [INT8](https://arxiv.org/abs/2208.07339) techniques during training.\n",
    "\n",
    "Furthermore, due to the long training time required and without extensive hyperparameter tuning, it's not necessary to overly concern yourself if the training results are not optimal. The main purpose of this exercise is to provide practical implementation experience with PyTorch Lightning and LLM training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6nHL1B4IfCK"
   },
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVJmvkuSIx-R"
   },
   "source": [
    "### Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8if2QWeQIQgt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU \\\n",
    "  accelerate \\\n",
    "  bitsandbytes \\\n",
    "  datasets \\\n",
    "  lightning \\\n",
    "  peft \\\n",
    "  sentencepiece \\\n",
    "  transformers \\\n",
    "  wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGintAIhFWqd"
   },
   "source": [
    "### Pre-download the LLaMA weights\n",
    "\n",
    "<!-- 正規的下載渠道應該要先填寫 [此表格](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form) 並等待 Meta 批准，才得以下載 LLaMA 的原始權重，並再得到原始權重後使用 HuggingFace 的轉換腳本轉換成 HuggingFace 的格式，但為了教學方便我們直接從 [這裡](https://huggingface.co/huggyllama/llama-7b) 下載已轉換的 LLaMA 權重 -->\n",
    "\n",
    "The official download channel requires you to first fill out [this form](https://docs.google.com/forms/d/e/1FAIpQLSfqNECQnMkycAp2jP4Z9TFX0cGR4uf7b_fBxjY_OjhJILlKGA/viewform?usp=send_form) and await approval from Meta. Only after obtaining permission from Meta can you download the raw weights of LLaMA. After acquiring the raw weights, you can use HuggingFace's conversion script to convert them into the HuggingFace format. However, for the sake of convenience in teaching, we directly download the converted LLaMA weights from [here](https://huggingface.co/huggyllama/llama-7b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j454fRSDFWqd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-09 13:26:22.811948: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-08-09 13:26:23.369665: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-08-09 13:26:23.369737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2023-08-09 13:26:23.369747: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab4674dca2243ae864408c692bad0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/joeyliang/112新生訓練/Day5/llama-7b'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import lightning as L\n",
    "from transformers import PreTrainedTokenizer\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download('huggyllama/llama-7b', local_dir='llama-7b', ignore_patterns='*safetensors*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO3cPo_wJ37t"
   },
   "source": [
    "### Login to Weights & Bias\n",
    "\n",
    "1. Sign-up for Weights & Bias if you don't have an account. https://wandb.ai\n",
    "2. Run the cell below and follow the steps to login.\n",
    "3. If you have successfully logged in, the second execution of the command will display your username.\n",
    "\n",
    "    ```\n",
    "    wandb: Currently logged in as: xxx. Use `wandb login --relogin` to force relogin\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mKYcm2cLInxk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoeyliang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoeyliang\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "!wandb login\n",
    "!wandb login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vuRl2OVInbo"
   },
   "source": [
    "## Data Preparation\n",
    "\n",
    "<!-- 這裡撰寫了兩個類別用來實作資料處理相關的邏輯 -->\n",
    "\n",
    "Two classes have been written here to implement the logic related to data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtcr87e4FWqd"
   },
   "source": [
    "### `DataCollatorForSupervisedFineTuning`\n",
    "\n",
    "<!-- 可以發現它其實就是 `DataLoader` 所需的 `collate_fn`，我們在此將原始資料套上模板，再進行斷詞等動作，最終轉換成模型所需的張量 -->\n",
    "\n",
    "It can be observed that this is essentially the `collate_fn` required for the DataLoader. Here, we apply a template to the raw data, perform tokenization, and other actions, ultimately transforming it into the tensors required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6CyOq1k9FWqe"
   },
   "outputs": [],
   "source": [
    "class DataCollatorForSupervisedFineTuning:\n",
    "  def __init__(self, tokenizer: PreTrainedTokenizer):\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    assert 'pad_token' in self.tokenizer.special_tokens_map\n",
    "    assert self.tokenizer.padding_side == 'right'\n",
    "\n",
    "    self.template = 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n'\n",
    "    self.template_wo_input = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n'\n",
    "\n",
    "  def get_prompt(self, x: dict[str, str], with_output: bool):\n",
    "    if not x['input']:\n",
    "      prompt = self.template_wo_input.format_map(x)\n",
    "    else:\n",
    "      prompt = self.template.format_map(x)\n",
    "\n",
    "    if with_output:\n",
    "      prompt += x['output'] + self.tokenizer.eos_token\n",
    "\n",
    "    return prompt\n",
    "\n",
    "  def __call__(self, batch: list):\n",
    "    batch_text = []\n",
    "    batch_prompt_length = []\n",
    "    for x in batch:\n",
    "      prompt = self.get_prompt(x, with_output=False)\n",
    "      prompt_length = self.tokenizer(prompt, return_length=True)['length']\n",
    "      batch_prompt_length.append(prompt_length)\n",
    "      batch_text.append(self.get_prompt(x, with_output=True))\n",
    "\n",
    "    batch_encoding = self.tokenizer(batch_text, return_tensors='pt', padding=True)\n",
    "    batch_labels = batch_encoding['input_ids'].masked_fill(batch_encoding['input_ids'] == self.tokenizer.pad_token_id, -100)\n",
    "    for i, prompt_length in enumerate(batch_prompt_length):\n",
    "      batch_labels[i, :prompt_length] = -100 # Mask out the prompt to only train on the output\n",
    "\n",
    "    return {\n",
    "      **batch_encoding,\n",
    "      'labels': batch_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_b7wQFp4FWqe"
   },
   "source": [
    "### `DataModuleForSupervisedFineTuning`\n",
    "\n",
    "As you can see, it inherits from `LightningDataModule`, so it's actually an API provided by PyTorch Lightning. However, this aspect was not mentioned in the presentation.\n",
    "\n",
    "Its main purpose is to encapsulate all the logic related to data processing, including preprocessing, data loading, and data splitting. Here's a simple demonstration of its usage. For detailed API information, you can refer to the [documentation](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningDataModule.html#lightning.pytorch.core.LightningDataModule)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ubljd-MkIPdi"
   },
   "outputs": [],
   "source": [
    "class DataModuleForSupervisedFineTuning(L.LightningDataModule):\n",
    "  @property\n",
    "  def dataloader_kwargs(self):\n",
    "    return dict(\n",
    "      batch_size=self.hparams.batch_size,\n",
    "      num_workers=self.hparams.num_workers,\n",
    "      pin_memory=self.hparams.pin_memory,\n",
    "    )\n",
    "\n",
    "  def __init__(\n",
    "    self,\n",
    "    tokenizer: PreTrainedTokenizer,\n",
    "    data_path: str,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 1,\n",
    "    pin_memory: bool = True,\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.save_hyperparameters(ignore=['tokenizer'])\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def setup(self, stage: str | None = None):\n",
    "    self.dataset = load_dataset(self.hparams.data_path)['train']\n",
    "    self.dataset = self.dataset.train_test_split(0.1, seed=42)\n",
    "    self.dataset['val'] = self.dataset.pop('test')\n",
    "\n",
    "  def train_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.dataset['train'],\n",
    "      shuffle=True,\n",
    "      collate_fn=DataCollatorForSupervisedFineTuning(self.tokenizer),\n",
    "      **self.dataloader_kwargs\n",
    "    )\n",
    "\n",
    "  def val_dataloader(self):\n",
    "    return DataLoader(\n",
    "      self.dataset['val'],\n",
    "      shuffle=False,\n",
    "      collate_fn=DataCollatorForSupervisedFineTuning(self.tokenizer),\n",
    "      **self.dataloader_kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1X7LQtAFWqe"
   },
   "source": [
    "## Write The Lightning Module\n",
    "\n",
    "<!-- !!此處需修改!!\n",
    "\n",
    "需求:\n",
    "1. 儲存超參數\n",
    "2. 設定優化器為 `bitsandbytes.optim.AdamW8bit`，並將超參數 `learning_rate` 傳遞給優化器\n",
    "3. 定義訓練步的邏輯，並紀錄訓練 loss 為 `Loss/Train`\n",
    "4. 定義驗證步的邏輯，並紀錄驗證 loss 為 `Loss/Val`\n",
    "\n",
    "提示:\n",
    "- [`LightningModule`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule)\n",
    "- 在 `LightningModule` 中，使用 `self.hparams` 來存取超參數\n",
    "- 計算 loss\n",
    "    ```python3\n",
    "    output = self.model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        labels=batch['labels'],\n",
    "        use_cache=False,\n",
    "    )\n",
    "    loss = output.loss\n",
    "    ``` -->\n",
    "\n",
    "!! Modifications Required Here !!\n",
    "\n",
    "Requirements:\n",
    "1. Save hyperparameters.\n",
    "2. Set the optimizer to `bitsandbytes.optim.AdamW8bit` and pass the hyperparameter `learning_rate` to the optimizer.\n",
    "3. Define the training step logic and record the training loss as `Loss/Train`.\n",
    "4. Define the validation step logic and record the validation loss as `Loss/Val`.\n",
    "\n",
    "Hints:\n",
    "- [`LightningModule`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.core.LightningModule.html#lightning.pytorch.core.LightningModule)\n",
    "- Inside the `LightningModule`, use `self.hparams` to access hyperparameters.\n",
    "- Compute loss as follows:\n",
    "    ```python3\n",
    "    output = self.model(\n",
    "        input_ids=batch['input_ids'],\n",
    "        attention_mask=batch['attention_mask'],\n",
    "        labels=batch['labels'],\n",
    "        use_cache=False,\n",
    "    )\n",
    "    loss = output.loss\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "D3lGgl5SNKUt"
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, get_peft_model_state_dict, set_peft_model_state_dict\n",
    "\n",
    "\n",
    "class LightningModuleForSupervisedFineTuning(L.LightningModule):\n",
    "  def __init__(\n",
    "    self,\n",
    "    model_path: str,\n",
    "    lora_r: int,\n",
    "    lora_alpha: int,\n",
    "    lora_dropout: float,\n",
    "    lora_target_modules: list[str],\n",
    "    learning_rate: float\n",
    "  ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.tokenizer = LlamaTokenizer.from_pretrained(model_path, legacy=False, pad_token='<pad>')\n",
    "\n",
    "    self.model = LlamaForCausalLM.from_pretrained(\n",
    "      model_path,\n",
    "      torch_dtype=torch.half,\n",
    "      low_cpu_mem_usage=True,\n",
    "      load_in_8bit=True,\n",
    "      device_map={'': 0}\n",
    "    )\n",
    "    self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "    self.model = prepare_model_for_kbit_training(self.model, use_gradient_checkpointing=True)\n",
    "    self.model = get_peft_model(self.model, LoraConfig(\n",
    "      task_type='CAUSAL_LM',\n",
    "      r=lora_r,\n",
    "      lora_alpha=lora_alpha,\n",
    "      lora_dropout=lora_dropout,\n",
    "      target_modules=lora_target_modules,\n",
    "    ))\n",
    "\n",
    "  def state_dict(self, **kwargs):\n",
    "    return get_peft_model_state_dict(self.model, self.model.state_dict(**kwargs))\n",
    "\n",
    "  def load_state_dict(self, state_dict, strict: bool = True):\n",
    "    return set_peft_model_state_dict(self.model, state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RwDaA-UiFWqe"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AMA8AyyFWqe"
   },
   "source": [
    "### Construct Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05Rl5NvzFWqe"
   },
   "outputs": [],
   "source": [
    "model = LightningModuleForSupervisedFineTuning(\n",
    "    model_path='llama-7b',\n",
    "    lora_r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    lora_target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zSxxXkKFWqe"
   },
   "source": [
    "### Construct Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0GHhR--FWqe"
   },
   "outputs": [],
   "source": [
    "datamodule = DataModuleForSupervisedFineTuning(\n",
    "    model.tokenizer,\n",
    "    data_path='yahma/alpaca-cleaned',\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uYq5q6L7FWqe"
   },
   "source": [
    "### Set-up the Logger\n",
    "\n",
    "<!-- !!此處須修改!!\n",
    "\n",
    "需求：\n",
    "- 使用 Wandb 作為 Logger\n",
    "- 設定此次實驗(訓練)要儲存在哪個 project\n",
    "- 為此次實驗(訓練)設定一個名字\n",
    "\n",
    "提示：\n",
    "[WandbLogger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb) -->\n",
    "\n",
    "!! Modification Required Here !!\n",
    "\n",
    "Requirements:\n",
    "- Use Wandb as the logger.\n",
    "- Set which project to save this run to.\n",
    "- Assign a name to this run.\n",
    "\n",
    "Hint:\n",
    "- [WandbLogger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hVxzPj2FWqe"
   },
   "outputs": [],
   "source": [
    "logger = WandbLogger(project=\"llama-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxuExa0JFWqf"
   },
   "source": [
    "### Set-up Callbacks\n",
    "\n",
    "<!-- !!此處須修改!!\n",
    "\n",
    "需求：\n",
    "1. 紀錄學習率\n",
    "2. 在 validation loss 沒有持續下降時自動停止訓練\n",
    "3. 每 1 epoch 儲存 1 個存檔點，並且每個 epoch 的存檔點都保留\n",
    "4. 每 500 steps 儲存 1 個存檔點，但只保留最新的存檔點\n",
    "5. 保留 1 個 validation loss 最低的存檔點，並將檔名設為 `val_loss=xxx`，`xxx` 為當下的 validation loss\n",
    "\n",
    "提示：\n",
    "- https://lightning.ai/docs/pytorch/stable/api_references.html#callbacks\n",
    "- 5\\. 會需要用到 `auto_insert_metric_name` 和 `filename` -->\n",
    "\n",
    "!! Modification Required Here !!\n",
    "\n",
    "Requirements:\n",
    "1. Record the learning rate.\n",
    "2. Automatically stop training when the validation loss does not decrease continuously.\n",
    "3. Save 1 checkpoint per 1 epoch, and retain all checkpoints for each epoch.\n",
    "4. Save 1 checkpoint every 500 steps, but only retain the latest checkpoint.\n",
    "5. Keep the checkpoint with the lowest validation loss, and name the file as `val_loss=xxx`, where `xxx` is the current validation loss.\n",
    "\n",
    "Hints:\n",
    "- https://lightning.ai/docs/pytorch/stable/api_references.html#callbacks\n",
    "- Requirement 5 will require using `auto_insert_metric_name` and `filename`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1unAPVPhFWqf"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "checkpoint_dir = 'check_point'\n",
    "callbacks = [\n",
    "    LearningRateMonitor(logging_interval='epoch'),\n",
    "    EarlyStopping(monitor='val_loss', patience=5, mode='min'),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename='checkpoint_epoch_{epoch:03d}',\n",
    "        save_top_k=-1,  # Save all checkpoints per epoch\n",
    "        verbose=True,\n",
    "        every_n_train_steps=500  # Save checkpoint per 500 steps\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        dirpath=checkpoint_dir,\n",
    "        filename='{val_loss:.4f}-{epoch}',\n",
    "        save_top_k=1,  # Save only the best validation loss checkpoint\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        auto_insert_metric_name=False  # Disable auto-insertion of metric name\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuo67basFWqf"
   },
   "source": [
    "### Set-up the Trainer\n",
    "\n",
    "<!-- !!此處須修改!!\n",
    "\n",
    "需求：\n",
    "1. 使用 FP16 混合精度訓練\n",
    "2. 使用 Gradient Clipping，數值隨意\n",
    "3. 使用 Gradient Accumulation，數值隨意\n",
    "4. 設定最多訓練幾 epoch，數值隨意\n",
    "5. 設定每幾步進行 1 次 validation，數值隨意\n",
    "\n",
    "提示：\n",
    "- [`Trainer`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer) -->\n",
    "\n",
    "!! Modification Required Here !!\n",
    "\n",
    "Requirements:\n",
    "1. Use FP16 mixed precision training.\n",
    "2. Apply gradient clipping with an arbitrary value.\n",
    "3. Enable gradient accumulation with an arbitrary value.\n",
    "4. Set the maximum number of training epochs with an arbitrary value.\n",
    "5. Define how often to perform validation every few steps with an arbitrary value.\n",
    "\n",
    "Hint:\n",
    "- [`Trainer`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html#lightning.pytorch.trainer.trainer.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NbMGabUcOp_9"
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xOU25UbyFWqf"
   },
   "source": [
    "### Start Training\n",
    "\n",
    "<!-- 記得要去 [W&B](https://wandb.ai) 觀察訓練過程並自己操作看看 -->\n",
    "Remember to visit [W&B](https://wandb.ai) to observe the training process and try it out for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U7h7IuArFWqf"
   },
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTCYkWSXFWqf"
   },
   "source": [
    "## Test the trained model\n",
    "\n",
    "Hint: You might want to restart the kernel to free the GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qa-pycdiFWqf"
   },
   "outputs": [],
   "source": [
    "ckpt_path = '' # Choose a checkpoint file\n",
    "model = LightningModuleForSupervisedFineTuning.load_from_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WyH4DjXaFWqf"
   },
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "\n",
    "def generate(model: LightningModuleForSupervisedFineTuning, prompt: str):\n",
    "    x = model.tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    l = x['input_ids'].size(1)\n",
    "    x = model.model.generate(**x, generation_config=GenerationConfig(max_new_tokens=32))\n",
    "    x = x[:, l:]\n",
    "    x = model.tokenizer.batch_decode(x, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ph2gmUDfFWqf"
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nRearrange the following sentence to make the sentence more interesting.\\n\\n### Input:\\nShe left the party early\\n\\n### Response:\\n',\n",
    "    'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nLet \\n f(x) = {[ -x - 3 if x ≤ 1,; x/2 + 1 if x > 1. ].\\nFind the sum of all values of x such that f(x) = 0.\\n\\n### Response:\\n',\n",
    "    'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nCompose a haiku poem about a summer day.\\n\\n### Response:\\n',\n",
    "    'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat methods can be used to improve the accuracy of machine learning models?\\n\\n### Response:\\n',\n",
    "    'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nFill in the blanks to complete the sentence.\\n\\n### Input:\\nGlobal warming can be reversed by reducing ________ and __________.\\n\\n### Response:\\n'\n",
    "]\n",
    "\n",
    "for p in prompts:\n",
    "    print(p)\n",
    "    print(generate(model, p) + '\\n')\n",
    "    print('=' * 100)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
